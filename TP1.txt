Attention:
- Do not edit this file in text editors like Word. Use a plain text editor only. In case of doubt, you can use Spyder as a text editor.
- Do not change the structure of this file. Just fill in your answers in the places provided (After the R#: tag).
- You can add lines in the spaces for your answers but your answers should be brief and straight to the point.

QUESTIONS:

Q1: Considering the data provided, explain the need to standardize the attribute values.
R1: Due to the fact that the feature values can vary too much, with some values significantly smaller than others, there is a need to
uniformize the feature values, so that the differences between these values are smaller.


Q2: Explain how you calculated the parameters for standardization and how you used them in the test set.
R2: The standardization procedure required us to calculate the mean and standard deviation of the training set, which we have used to
standardize both the training and test sets, using the formula (Features - training_means) / training_stds


Q3: Explain how you calculated the prior probability of an example belonging to a class (the probability before taking into account the attribute values ​​of the example) in your Naïve Bayes classifier implementation. You may include a relevant piece of your code if this helps you explain.
R3: To calculate the prior probability of an example beloging to a class we've counted the number of examples with class 0 and the
number of examples with class 1, and divided these values by the total number of examples, such as: np.sum(Y == Class) / len(Y)


Q4: Explain how your Naïve Bayes classifier predicts the class to which a test example belongs. You may include a relevant piece of your code if this helps you explain.
R4: In order to predict, we've calculated the logarithm of the prior probability of an example beloging to a class with the training set, and afterwards we've used the
training set examples to fit the kernel density estimators (of both classes), for each feature, passing the test set to the score_samples method of each estimator to obtain
the logarithmic probability of the feature samples belonging to a class, adding these results to the logarithm of the prior probability of each class. 

// the prior probability of each class
c_test_0 = np.log(np.sum(train_y == 0) / len(train_y))
c_test_1 = np.log(np.sum(train_y == 1) / len(train_y))

// for each feature *feat*:
test_dens = self._kde(self.train_0[:,[feat]], test[:,[feat]], bw)
c_test_0 += test_dens

test_dens = self._kde(self.train_1[:,[feat]], test[:,[feat]], bw)
c_test_1 += test_dens

Finally we've used argmax to predict the class the test samples belong to.


Q5: Explain the effect of the bandwidth parameter on your classifier.
R5: While lower values of the bandwidth overfit our kernel density estimator, higher values underfit the kde. 


Q6: Explain what effect the gamma parameter has on the SVM classifier.
R6: A higher gamma value will produce a smaller class region, whilst a lower gamma value will produce a larger region. We can look to the gamma value as the inverse
of the radius of the class, for each training sample.


Q7: Explain how you determined the best bandwidth and gamma parameters for your classifier and the SVM classifier. You may include a relevant piece of your code if this helps you explain.
R7: To determine the best parameters, we've used the StratifiedKFold from scikit-learn. With the stratified folding and 5 folds, we can divide our data into 5 sets, observing the proportions
between the training and cross-validation sets. With both of these sets we proceeded to calculate the error of the predictions performed by the training and cross-validation sets.
When a lower cross-validation error is achieved we save this result, until another best value is discovered. By the end, we get the values of the parameters with the lowest cross-validation
error.


Q8: Explain how you obtained the best hypothesis for each classifier after optimizing all parameters.
R8: To obtain the best hypothesis for the Naive Bayes w/ KDE classifier we've passed the best parameters to the Kernel Densisty Estimators, fitting these estimators with our training set.
For the Gaussian Naive Bayes, we've simply fitted the classifier with our training set, while with the SVM classifier we've passed the optimal parameters to the classifier, fitting this classifier
with the training set once more.


Q9: Show the best parameters, the estimate of the true error for each hypothesis you obtained (your classifier and the two provided by the library), the ranges in the expected number of errors given by the approximate normal test, the McNemar test values, and discuss what you can conclude from this.
R9: The best bandwidth obtained for the KDE was 0.1 with a validation error of 0.61.
The best gamma obtained for the SVM classifier was 3.4 (with C = 1), with a validation error of 0.03.

The test errors for each hypothesis were the following:
Naive Bayes w/ KDE: 0.067
Naive Bayes w/ Gaussian: 0.095
SVM: 0.041

The ranges of the approximate normal tests were:
Naive Bayes w/ KDE: 83 +- 17
Naive Bayes w/ Gaussian: 118 +- 20
SVM: 51 +- 14

While the McNemar's test values were:
Naive Bayes w/ KDE vs Naive Bayes w/ Gaussian: 18.35
Naive Bayes w/ KDE vs SVM: 15.02
Naive Bayes w/ Gaussian vs SVM: 39.96

From this we can conclude that the SVM classifier is the best one, having a test error range of 51 +- 14, with the McNemar's test values >= 3.84 that means that the difference is significant.
The second best classifier was the Naive Bayes w/ KDE, having a test error range of 83 +- 17 with a McNemar's test value against the gaussian classifier of 18.35 (>= 3.84).
The worst classifier was the Naive Bayes w/ Gaussian, having a test error range of 118 +- 20, larger than the other classifiers ranges, with all the McNemar's tests showing a significant difference. 


Q10: (Optional) Show the estimate of the true error of the optimized SVM classifier (if you did the optional part of the work) and discuss whether it was worth doing this optimization. If you did not do the optional part leave this answer blank.
R10: For the optimized SVM classifier we've obtained a C value of 201, with a gamma value of 0.4. This optimized classifier obtained a test error of ~0.043, which is 0.001 higher than the error
obtained without the optimization, thus we can conclude that this optimization was not worth the additional computing time it requires, for our dataset.

